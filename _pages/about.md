---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---




I co-founded [Voyage AI](https://www.voyageai.com) and led its research to develop the best embeddings models and rerankers for semantic search and information retrieval in the industry. I did my PhD at Stanford University, affiliated with [Stanford AI Lab](https://ai.stanford.edu) and [Stanford NLP group](https://nlp.stanford.edu). My research interests broadly lie in language models, pre-training, multimodal, and reasoning.

## News

Checkout our latest project [MoCa](https://haon-chen.github.io/MoCa/), which scales multimodal embedding models with unlabeled interleaved multimodal data. 


## Language Models

<b>[Synthetic bootstrapped pretraining](https://arxiv.org/abs/2509.15248)</b> <br> Zitong Yang, Aonan Zhang, Hong Liu, Tatsunori Hashimoto, Emmanuel Cand√®s, Chong Wang, Ruoming Pang. <br> <i>ArXiv 2509.15248</i> [[Twitter]](https://x.com/zitongyang0/status/1970129028536484089?s=46)

<b>[MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings](https://arxiv.org/abs/2506.23115)</b> <br> Haonan Chen, Hong Liu, Yuping Luo, Liang Wang, Nan Yang, Furu Wei, Zhicheng Dou. <br> <i>ArXiv 2506.23115</i> [[Code]](https://github.com/haon-chen/MoCa)[[Twitter]](https://x.com/hongliu9903/status/1940084337497793014?s=46)

<b>[Chain of Thought Empowers Transformers to Solve Inherently Serial Problems](https://arxiv.org/abs/2402.12875)</b> <br> Zhiyuan Li, Hong Liu, Denny Zhou, Tengyu Ma. <br> <i>ICLR 2024</i> [[Twitter]](https://x.com/denny_zhou/status/1835761801453306089)


<b>[Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training](https://arxiv.org/pdf/2305.14342)</b> <br> Hong Liu, Zhiyuan Li, David Hall, Percy Liang, Tengyu Ma. <br> <i>ICLR 2024</i> [[Code]](https://github.com/Liuhong99/Sophia) [[Twitter]](https://twitter.com/tengyuma/status/1661412995430219786)

<b>[Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models](https://arxiv.org/pdf/2210.14199)</b> <br> Hong Liu, Sang Michael Xie, Zhiyuan Li, Tengyu Ma. <br><i>International Conference on Machine Learning (ICML), 2023, <b>Oral</b>.</i> [[Code]](https://github.com/Liuhong99/implicitbiasmlmcode) [[Twitter]](https://twitter.com/tengyuma/status/1593328919624617985) 

<b>[Self-supervised Learning is More Robust to Dataset Imbalance](https://arxiv.org/pdf/2110.05025)</b> <br> Hong Liu, Jeff Z. HaoChen, Adrien Gaidon, Tengyu Ma. <br><i>International Conference on Learning Representations (ICLR), 2022, <b>Spotlight</b>.</i> [[Code]](https://github.com/Liuhong99/Imbalanced-SSL) [[Twitter]](https://twitter.com/tengyuma/status/1448335906524606464) 


## Domain Adaptation and Transfer Learning

<b>[Cycle Self-Training for Domain Adaptation](https://proceedings.neurips.cc/paper/2021/file/c1fea270c48e8079d8ddf7d06d26ab52-Paper.pdf)</b> <br> Hong Liu, Jianmin Wang, Mingsheng Long. <br><i>Neural Information Processing Systems (NeurIPS), 2021.</i> [[Code]](https://github.com/Liuhong99/CST) 

<b>[Learning to Adapt to Evolving Domains](https://proceedings.neurips.cc/paper/2020/file/fd69dbe29f156a7ef876a40a94f65599-Paper.pdf)</b> <br> Hong Liu, Mingsheng Long, Jianmin Wang, Yu Wang. <br><i>Neural Information Processing Systems (NeurIPS), 2020.</i> 

<b>[Meta-learning Transferable Representations with a Single Target Domain](https://arxiv.org/pdf/2011.01418)</b> <br> Hong Liu, Jeff Z. HaoChen, Colin Wei, Tengyu Ma <br><i>Arxiv 2011.01418.</i> 

<b>[Towards Understanding the Transferability of Deep Representations](https://arxiv.org/pdf/1909.12031)</b> <br> Hong Liu, Mingsheng Long, Jianmin Wang, Michael Jordan. <br><i>Arxiv 1909.12031.</i> 

<b>[Transferable adversarial training: A general approach to adapting deep classifiers](http://proceedings.mlr.press/v97/liu19b/liu19b.pdf)</b> <br> Hong Liu, Mingsheng Long, Jianmin Wang, Michael Jordan. <br><i>International Conference on Machine Learning (ICML), 2019, <b>Long Talk</b>.</i> [[Code]](https://github.com/Liuhong99/Transferable-Adversarial-Training) 

<b>[Separate to Adapt: Open Set Domain Adaptation via Progressive Separation](http://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Separate_to_Adapt_Open_Set_Domain_Adaptation_via_Progressive_Separation_CVPR_2019_paper.pdf)</b> <br> Hong Liu, Zhangjie Cao, Mingsheng Long, Jianmin Wang, Qiang Yang. <br> <i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.</i> [[Code]](https://github.com/Liuhong99/Separate_to_Adapt) 